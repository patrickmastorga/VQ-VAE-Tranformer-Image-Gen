{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7fbbafb2",
      "metadata": {
        "id": "7fbbafb2"
      },
      "source": [
        "# Training Notebook for VQ-VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b52c75",
      "metadata": {
        "id": "e5b52c75"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R-nf_hUh88R5",
      "metadata": {
        "id": "R-nf_hUh88R5"
      },
      "source": [
        "### Load CelebA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "URPH1QoEOcWg",
      "metadata": {
        "id": "URPH1QoEOcWg"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gWJUEtAeQhhZ",
      "metadata": {
        "id": "gWJUEtAeQhhZ"
      },
      "outputs": [],
      "source": [
        "# access the kaggle.json API key from the main folder of your google drive\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# download the dataset from kaggle\n",
        "!kaggle datasets download -d zuozhaorui/celeba\n",
        "!mkdir ./data\n",
        "!unzip -q celeba.zip -d ./data/celeba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AKe80mkz498P",
      "metadata": {
        "id": "AKe80mkz498P"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "class CelebATransform:\n",
        "    '''\n",
        "    Crops around the face and resizes to 64x64. Output is a tensor of shape (3, 64, 64) scaled to [0, 1]\n",
        "    '''\n",
        "    def __call__(self, img):\n",
        "        img = torchvision.transforms.functional.crop(img, top=60, left=25, height=128, width=128)\n",
        "        img = torchvision.transforms.functional.resize(img, (64, 64))\n",
        "        img = torchvision.transforms.functional.to_tensor(img)\n",
        "        return img\n",
        "celeba = torchvision.datasets.ImageFolder(root='./data/celeba', transform=CelebATransform())\n",
        "\n",
        "# visualize\n",
        "grid_x = 5\n",
        "grid_y = 4\n",
        "\n",
        "samples = torch.stack([celeba[i][0] for i in range(grid_x*grid_y)])\n",
        "\n",
        "img = torchvision.utils.make_grid(samples, grid_x, normalize=True)\n",
        "plt.title(f'Sample Images')\n",
        "plt.axis('off')\n",
        "plt.imshow(img.permute(1,2,0).cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4WpiSXGJ8193",
      "metadata": {
        "id": "4WpiSXGJ8193"
      },
      "source": [
        "### Import model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14cbe8f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# clone the github repository containing the VQ-VAE model and import models\n",
        "!git clone https://github.com/patrickmastorga/VQ-VAE-Tranformer-Image-Gen.git\n",
        "\n",
        "os.chdir('VQ-VAE-Tranformer-Image-Gen/VQ')\n",
        "from model import Encoder, Decoder, Quantizer, VQ_VAE, EMBEDDING_DIM\n",
        "os.chdir('../../')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tr-m5WkYRlx7",
      "metadata": {
        "id": "Tr-m5WkYRlx7"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pg7sWQCJ3IZ5",
      "metadata": {
        "id": "pg7sWQCJ3IZ5"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "BETA = 0.25\n",
        "\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/vq_models'\n",
        "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'checkpoint.pt')\n",
        "LOAD_FROM_CHECKPOINT = False\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# initialize dataloader, models, and optimizer for training\n",
        "dataloader = torch.utils.data.DataLoader(celeba, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder().to(device)\n",
        "quantizer = Quantizer().to(device)\n",
        "model = VQ_VAE(encoder, decoder, quantizer).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "training_losses = []\n",
        "training_steps = 0\n",
        "running_loss = 0.0\n",
        "\n",
        "# load from checkpoint\n",
        "if LOAD_FROM_CHECKPOINT:\n",
        "    if not os.path.exists(CHECKPOINT_PATH):\n",
        "        print(f'WARNING: Checkpoint not found at {CHECKPOINT_PATH}!')\n",
        "    else:\n",
        "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "        training_steps = checkpoint['training_steps']\n",
        "        training_losses = checkpoint['training_losses']\n",
        "        running_loss = checkpoint['running_loss']\n",
        "        torch.set_rng_state(checkpoint['cpu_rng_state'])\n",
        "        if torch.cuda.is_available() and 'cuda_rng_state' in checkpoint:\n",
        "            torch.cuda.set_rng_state(checkpoint['cuda_rng_state'])\n",
        "\n",
        "        print(f'Checkpoint loaded. Resuming from training step {training_steps}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Eps9W3v06jRc",
      "metadata": {
        "collapsed": true,
        "id": "Eps9W3v06jRc"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 1\n",
        "LOG_INTERVAL = 128\n",
        "SAVE_INTERVAL = 10000\n",
        "\n",
        "total_steps = training_steps + len(dataloader) * EPOCHS\n",
        "\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    for batch in dataloader:\n",
        "        # training step\n",
        "        optimizer.zero_grad()\n",
        "        images, _ = batch\n",
        "        images = images.to(device)\n",
        "\n",
        "        loss = model(images)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        training_steps += 1\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # keep track of loss and epoch progress\n",
        "        if training_steps % LOG_INTERVAL == 0:\n",
        "            avg_loss = running_loss / LOG_INTERVAL\n",
        "            running_loss = 0.0\n",
        "            training_losses.append((training_steps, avg_loss))\n",
        "            print(f'TRAINING Step [{training_steps}/{total_steps}], Loss: {avg_loss:.1f}')\n",
        "        \n",
        "        if training_steps % SAVE_INTERVAL == 0:\n",
        "            checkpoint = {\n",
        "                'training_steps': training_steps,\n",
        "                'model_state': model.state_dict(),\n",
        "                'optimizer_state': optimizer.state_dict(),\n",
        "                'training_losses': training_losses,\n",
        "                'running_loss': running_loss,\n",
        "                'cpu_rng_state': torch.get_rng_state(),\n",
        "            }\n",
        "            if torch.cuda.is_available():\n",
        "                checkpoint['cuda_rng_state'] = torch.cuda.get_rng_state()\n",
        "\n",
        "            torch.save(checkpoint, CHECKPOINT_PATH)\n",
        "            print(f'Checkpoint saved at step {training_steps} to {CHECKPOINT_PATH}')\n",
        "\n",
        "    # visualize reconstructions\n",
        "    samples, _ = next(iter(dataloader))\n",
        "    samples = samples[:5].to(device)\n",
        "\n",
        "    model.eval()\n",
        "    reconstructed = model.reconstruct(samples)\n",
        "    model.train()\n",
        "\n",
        "    img = torchvision.utils.make_grid(torch.cat((samples, reconstructed), dim=0), 5, normalize=True)\n",
        "    plt.title(f'Reconstructions')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img.permute(1,2,0).cpu())\n",
        "\n",
        "print(f'Training complete.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
