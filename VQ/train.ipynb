{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fbbafb2"
      },
      "source": [
        "# Training Notebook for VQ-VAE"
      ],
      "id": "7fbbafb2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5b52c75"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import strftime"
      ],
      "id": "e5b52c75"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-nf_hUh88R5"
      },
      "source": [
        "### Load CelebA dataset"
      ],
      "id": "R-nf_hUh88R5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URPH1QoEOcWg"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "URPH1QoEOcWg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWJUEtAeQhhZ"
      },
      "outputs": [],
      "source": [
        "# access the kaggle.json API key from the main folder of your google drive\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# download the dataset from kaggle\n",
        "!kaggle datasets download -d zuozhaorui/celeba\n",
        "!mkdir ./data\n",
        "!unzip -q celeba.zip -d ./data/celeba"
      ],
      "id": "gWJUEtAeQhhZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKe80mkz498P"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "class CelebATransform:\n",
        "    '''\n",
        "    Crops around the face and resizes to 64x64. Output is a tensor of shape (3, 64, 64) scaled to [0, 1]\n",
        "    '''\n",
        "    def __call__(self, img):\n",
        "        img = torchvision.transforms.functional.crop(img, top=60, left=25, height=128, width=128)\n",
        "        img = torchvision.transforms.functional.resize(img, (64, 64))\n",
        "        img = torchvision.transforms.functional.to_tensor(img)\n",
        "        # img = torchvision.transforms.functional.normalize(img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        return img\n",
        "celeba = torchvision.datasets.ImageFolder(root='./data/celeba', transform=CelebATransform())\n",
        "\n",
        "# visualize\n",
        "grid_x = 5\n",
        "grid_y = 4\n",
        "\n",
        "samples = torch.stack([celeba[i][0] for i in range(grid_x*grid_y)])\n",
        "\n",
        "img = torchvision.utils.make_grid(samples, grid_x, normalize=True, value_range=(0, 1))\n",
        "plt.title(f'Sample Images')\n",
        "plt.axis('off')\n",
        "plt.imshow(img.permute(1,2,0).cpu())"
      ],
      "id": "AKe80mkz498P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WpiSXGJ8193"
      },
      "source": [
        "### Define models"
      ],
      "id": "4WpiSXGJ8193"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14cbe8f8"
      },
      "outputs": [],
      "source": [
        "# these are the hyperparameters used in the original VQ-VAE paper (see section 4.1)\n",
        "HIDDEN_CHANNELS = 256\n",
        "LATENT_DIM = 8 * 8\n",
        "EMBEDDING_DIM = 64\n",
        "NUM_EMBEDDINGS = 512\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    implementation of the residual block as described in section 4.1 of the original VQ-VAE paper\\\\\n",
        "    ReLU -> 3x3 conv -> ReLU -> 1x1 conv -> skip connection\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(HIDDEN_CHANNELS, HIDDEN_CHANNELS, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(HIDDEN_CHANNELS, HIDDEN_CHANNELS, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x + self.network(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    maps an 64x64 image tensor to a 8x8 latent tensor\\\\\n",
        "    downsample -> residual block -> downsample -> residual block -> downsample -> residual block -> 1x1 conv\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            # 64x64 image\n",
        "            nn.Conv2d(in_channels=3, out_channels=HIDDEN_CHANNELS, kernel_size=4, stride=2, padding=1),\n",
        "            # 32x32 hidden\n",
        "            ResidualBlock(),\n",
        "            nn.Conv2d(in_channels=HIDDEN_CHANNELS, out_channels=HIDDEN_CHANNELS, kernel_size=4, stride=2, padding=1),\n",
        "            # 16x16 hidden\n",
        "            ResidualBlock(),\n",
        "            nn.Conv2d(in_channels=HIDDEN_CHANNELS, out_channels=HIDDEN_CHANNELS, kernel_size=4, stride=2, padding=1),\n",
        "            # 8x8 hidden\n",
        "            ResidualBlock(),\n",
        "            nn.Conv2d(in_channels=HIDDEN_CHANNELS, out_channels=EMBEDDING_DIM, kernel_size=1),\n",
        "            # 8x8 latents\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.network(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    maps a 8x8 quantized latent tensor to an 64x64 image tensor (scaled to [0, 1])\\\\\n",
        "    1x1 conv -> residual block -> upsample -> residual block -> upsample -> residual block -> upsample\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            # 8x8 latents\n",
        "            nn.Conv2d(in_channels=EMBEDDING_DIM, out_channels=HIDDEN_CHANNELS, kernel_size=1),\n",
        "            # 8x8 hidden\n",
        "            ResidualBlock(),\n",
        "            nn.ConvTranspose2d(in_channels=HIDDEN_CHANNELS, out_channels=HIDDEN_CHANNELS, kernel_size=4, stride=2, padding=1),\n",
        "            # 16x16 hidden\n",
        "            ResidualBlock(),\n",
        "            nn.ConvTranspose2d(in_channels=HIDDEN_CHANNELS, out_channels=HIDDEN_CHANNELS, kernel_size=4, stride=2, padding=1),\n",
        "            # 32x32 hidden\n",
        "            ResidualBlock(),\n",
        "            nn.ConvTranspose2d(in_channels=HIDDEN_CHANNELS, out_channels=3, kernel_size=4, stride=2, padding=1),\n",
        "            # 64x64 image\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.network(x)\n",
        "\n",
        "class Quantizer(nn.Module):\n",
        "    \"\"\"\n",
        "    implementation of the codebook with nearnest neighbor lookup\\\\\n",
        "    if use_EMA=True, embeddings are learnt automatically as exponential moving averages of the encoder outputs assigned to them over minibatches (see Appendix A.1 of the original VQ-VAE paper)\\\\\n",
        "    otherwise, the embeddings are parameters to be learnt with gradient descent on the codebook loss (see section 3.2 of the original VQ-VAE paper)\n",
        "    \"\"\"\n",
        "    def __init__(self, use_EMA=False, batch_size=0, decay=0.99):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            use_EMA (bool): if True, use EMA updates to learn the codebook during training\n",
        "            batch_size (int): used to initialize the EMA running cluster counts/sums\n",
        "            decay (float): EMA decay parameter\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.use_EMA = use_EMA\n",
        "\n",
        "        # codebook dictionary\n",
        "        if not self.use_EMA:\n",
        "            self.register_parameter('e', nn.Parameter(torch.randn(NUM_EMBEDDINGS, EMBEDDING_DIM)))\n",
        "        else:\n",
        "            self.register_buffer('e', torch.randn(NUM_EMBEDDINGS, EMBEDDING_DIM))\n",
        "\n",
        "            # EMA running cluster counts and sums\n",
        "            self.decay = decay\n",
        "            expected_count = batch_size * LATENT_DIM / NUM_EMBEDDINGS\n",
        "            self.register_buffer('N', torch.full((NUM_EMBEDDINGS,), expected_count))\n",
        "            self.register_buffer('m', self.e.clone() * expected_count)\n",
        "\n",
        "    def nearest_neighbor_indices(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        64x64 image tensor -> 8x8 index tensor\n",
        "        \"\"\"\n",
        "        # flatten the embeddings along batch size, height, and width (B, embedding_dim, H, W) -> (BHW, embedding_dim)\n",
        "        B, _, H, W = x.shape\n",
        "        z_e_flat = x.permute(0, 2, 3, 1).reshape(-1, EMBEDDING_DIM)\n",
        "\n",
        "        # to calculate pairwise distance, use ||z - e||^2 = ||z||^2 - 2z*e + ||e||^2\n",
        "        with torch.no_grad():\n",
        "            dist = (\n",
        "                z_e_flat.pow(2).sum(dim=1, keepdim=True) # ||z||^2 (BHW, 1)\n",
        "                + self.e.pow(2).sum(dim=1).unsqueeze(0)  # ||e||^2 (1, NUM_EMBEDDING)\n",
        "                - 2 * z_e_flat @ self.e.T                # -2z*e   (BHW, NUM_EMBEDDING)\n",
        "            )\n",
        "        indices_flat = dist.argmin(1)                                   # (BHW,)\n",
        "        return indices_flat.view(B, H, W).permute(0, 1, 2).contiguous() # (B, H, W)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # flatten the embeddings along batch size, height, and width (B, embedding_dim, H, W) -> (BHW, embedding_dim)\n",
        "        B, _, H, W = x.shape\n",
        "        z_e_flat = x.permute(0, 2, 3, 1).reshape(-1, EMBEDDING_DIM)\n",
        "\n",
        "        # to calculate pairwise distance, use ||z - e||^2 = ||z||^2 - 2z*e + ||e||^2\n",
        "        with torch.no_grad():\n",
        "            dist = (\n",
        "                z_e_flat.pow(2).sum(dim=1, keepdim=True) # ||z||^2 (BHW, 1)\n",
        "                + self.e.pow(2).sum(dim=1).unsqueeze(0)  # ||e||^2 (1, NUM_EMBEDDING)\n",
        "                - 2 * z_e_flat @ self.e.T                # -2z*e   (BHW, NUM_EMBEDDING)\n",
        "            )\n",
        "        indices_flat = dist.argmin(1)\n",
        "\n",
        "        # EMA codebook update\n",
        "        if self.use_EMA and self.training:\n",
        "            # current minibatch cluster counts\n",
        "            n_i = torch.bincount(indices_flat, minlength=NUM_EMBEDDINGS).float()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # current minibatch cluster sums\n",
        "                m_i = torch.zeros_like(self.e)\n",
        "                m_i.index_add_(0, indices_flat, z_e_flat)\n",
        "\n",
        "                # EMA updates\n",
        "                self.N = self.decay * self.N + (1 - self.decay) * n_i\n",
        "                self.m = self.decay * self.m + (1 - self.decay) * m_i\n",
        "                self.e = self.m / (self.N.unsqueeze(1) + 1e-8)\n",
        "\n",
        "        z_q = nn.functional.embedding(indices_flat, self.e).view(B, H, W, EMBEDDING_DIM) # (B, H, W, embedding_dim)\n",
        "        return z_q.permute(0, 3, 1, 2).contiguous()                                      # (B, embedding_dim, H, W)\n",
        "\n",
        "class VQ_VAE(nn.Module):\n",
        "    \"\"\"\n",
        "        implements the encoder, decoder, and quantizer into a single model for training\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, quantizer: Quantizer, use_EMA=False, beta=0.25):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.quantizer = quantizer\n",
        "        self.use_EMA = use_EMA\n",
        "        self.beta = beta\n",
        "\n",
        "    def compute_indices(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        64x64 image tensor -> 8x8 index tensor (without computing gradients)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            x = self.encoder(x)\n",
        "            return self.quantizer.nearest_neighbor_indices(x)\n",
        "\n",
        "    def compute_latents(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        64x64 image tensor -> 8x8 quantized latent tensor (without computing gradients)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            x = self.encoder(x)\n",
        "            return self.quantizer(x)\n",
        "\n",
        "    def reconstruct(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        64x64 image tensor -> 64x64 reconstructed image tensor (without computing gradients)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            x = self.compute_latents(x)\n",
        "            return self.decoder(x)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor | None]:\n",
        "        \"\"\"\n",
        "        64x64 image tensor -> reconstruction_loss, commitment_loss, codebook_loss\\\\\n",
        "        if use_EMA=True, codebook_loss is None\n",
        "        \"\"\"\n",
        "        z_e = self.encoder(x)\n",
        "        z_q = self.quantizer(z_e)\n",
        "\n",
        "        # straight through estimator\n",
        "        z_q_st = z_e + (z_q - z_e).detach()\n",
        "        reconstructed = self.decoder(z_q_st)\n",
        "\n",
        "        # compute loss\n",
        "        reconstruction_loss = nn.functional.mse_loss(reconstructed, x)\n",
        "        commitment_loss = nn.functional.mse_loss(z_e, z_q.detach())\n",
        "        if self.use_EMA:\n",
        "            codebook_loss = nn.functional.mse_loss(z_e.detach(), z_q)\n",
        "            return reconstruction_loss, commitment_loss, codebook_loss\n",
        "        else:\n",
        "            return reconstruction_loss, commitment_loss, None"
      ],
      "id": "14cbe8f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr-m5WkYRlx7"
      },
      "source": [
        "### Train model"
      ],
      "id": "Tr-m5WkYRlx7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg7sWQCJ3IZ5"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "# initialize dataloader, models, and optimizer for training\n",
        "dataloader = torch.utils.data.DataLoader(celeba, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "quantizer = Quantizer(use_EMA=True, batch_size=BATCH_SIZE)\n",
        "model = VQ_VAE(encoder, decoder, quantizer, use_EMA=True).to(device)"
      ],
      "id": "pg7sWQCJ3IZ5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eps9W3v06jRc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "EPOCHS = 0\n",
        "LOG_INTERVAL = 100\n",
        "SAVE_INTERVAL = 1000\n",
        "BETA = 0.1\n",
        "\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/vq_models'\n",
        "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'checkpoint.pt')\n",
        "LOAD_FROM_CHECKPOINT = True\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "training_losses = []\n",
        "training_steps = 0\n",
        "running_losses = [0.0, 0.0, 0.0]\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# load from checkpoint\n",
        "if LOAD_FROM_CHECKPOINT:\n",
        "    if not os.path.exists(CHECKPOINT_PATH):\n",
        "        print(f'WARNING: Checkpoint not found at {CHECKPOINT_PATH}!')\n",
        "    else:\n",
        "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "        training_steps = checkpoint['training_steps']\n",
        "        training_losses = checkpoint['training_losses']\n",
        "        running_losses = checkpoint['running_losses']\n",
        "\n",
        "        print(f'Checkpoint loaded. Resuming from training step {training_steps}.')\n",
        "\n",
        "total_steps = training_steps + len(dataloader) * EPOCHS\n",
        "\n",
        "print(f'{strftime('%H:%M:%S')} Begin Training')\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    for batch in dataloader:\n",
        "        # training step\n",
        "        optimizer.zero_grad()\n",
        "        images, _ = batch\n",
        "        images = images.to(device)\n",
        "\n",
        "        reconstruction_loss, commitment_loss, codebook_loss = model(images)\n",
        "        if model.use_EMA:\n",
        "            loss = reconstruction_loss + BETA * commitment_loss + codebook_loss\n",
        "        else:\n",
        "            loss = reconstruction_loss + BETA * commitment_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        training_steps += 1\n",
        "\n",
        "        running_losses[0] += loss.item()\n",
        "        running_losses[1] += reconstruction_loss.item()\n",
        "        running_losses[2] += commitment_loss.item()\n",
        "\n",
        "        # keep track of loss and epoch progress\n",
        "        if training_steps % LOG_INTERVAL == 0:\n",
        "            avg_losses = [loss / LOG_INTERVAL for loss in running_losses]\n",
        "            running_losses = [0.0, 0.0, 0.0]\n",
        "            training_losses.append((training_steps, avg_losses))\n",
        "            with torch.no_grad():\n",
        "                p = model.quantizer.N / model.quantizer.N.sum() * 512\n",
        "                print(f'{strftime('%H:%M:%S')} TRAINING Step [{training_steps}/{total_steps}]; Loss: {avg_losses[0]:.4f}; Commitment: {avg_losses[2]:.4f}; Usage (min/med/max/dead): {p.min().item():.2f}, {p.median().item():.2f}, {p.max().item():.2f}, {torch.sum(p < 0.01) / 512 * 100:.0f}%')\n",
        "\n",
        "        if training_steps % SAVE_INTERVAL == 0:\n",
        "            checkpoint = {\n",
        "                'training_steps': training_steps,\n",
        "                'model_state': model.state_dict(),\n",
        "                'optimizer_state': optimizer.state_dict(),\n",
        "                'training_losses': training_losses,\n",
        "                'running_losses': running_losses,\n",
        "            }\n",
        "\n",
        "            torch.save(checkpoint, CHECKPOINT_PATH)\n",
        "            print(f'Checkpoint saved at step {training_steps} to {CHECKPOINT_PATH}')\n",
        "\n",
        "            # visualize reconstructions\n",
        "            samples, _ = next(iter(dataloader))\n",
        "            samples = samples[:5].to(device)\n",
        "\n",
        "            model.eval()\n",
        "            reconstructed = model.reconstruct(samples)\n",
        "            model.train()\n",
        "\n",
        "            img = torchvision.utils.make_grid(torch.cat((samples, reconstructed), dim=0), 5, normalize=True, value_range=(0, 1))\n",
        "            plt.title(f'Reconstructions at step {training_steps}')\n",
        "            plt.axis('off')\n",
        "            plt.imshow(img.permute(1,2,0).cpu())\n",
        "            plt.show()\n",
        "\n",
        "print(f'Training complete.')\n",
        "\n",
        "# Prepare loss data\n",
        "steps_list = [item[0] for item in training_losses]\n",
        "avg_losses_list = [item[1] for item in training_losses]\n",
        "\n",
        "steps = np.array(steps_list)\n",
        "losses = np.array(avg_losses_list)   # shape: (num_steps, 3)\n",
        "\n",
        "# Prepare reconstructions\n",
        "samples, _ = next(iter(dataloader))\n",
        "samples = samples[:10].to(device)\n",
        "\n",
        "model.eval()\n",
        "reconstructed = model.reconstruct(samples)\n",
        "model.train()\n",
        "\n",
        "img = torchvision.utils.make_grid(torch.cat((samples, reconstructed), dim=0), nrow=5, normalize=True, value_range=(0, 1))\n",
        "\n",
        "# Side-by-side plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Loss curves\n",
        "axes[0].plot(steps, losses[:, 0], label='Loss')\n",
        "axes[0].plot(steps, losses[:, 1], label='Reconstruction Loss')\n",
        "axes[0].plot(steps, losses[:, 2], label='Commitment Loss')\n",
        "\n",
        "axes[0].set_xlabel('Training Step')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Losses over Training Steps')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Reconstructions\n",
        "axes[1].imshow(img.permute(1, 2, 0).cpu())\n",
        "axes[1].set_title('Reconstructions')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "Eps9W3v06jRc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute CelebA embeddings"
      ],
      "metadata": {
        "id": "D3O6D1xNNbeI"
      },
      "id": "D3O6D1xNNbeI"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/vq_models'\n",
        "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'checkpoint.pt')\n",
        "LOAD_FROM_CHECKPOINT = True\n",
        "\n",
        "# load from checkpoint\n",
        "if LOAD_FROM_CHECKPOINT:\n",
        "    if not os.path.exists(CHECKPOINT_PATH):\n",
        "        print(f'WARNING: Checkpoint not found at {CHECKPOINT_PATH}!')\n",
        "    else:\n",
        "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state'])\n",
        "        print(f'Checkpoint loaded.')\n",
        "\n",
        "celeba_indices = []\n",
        "\n",
        "model.eval()\n",
        "for batch in tqdm(dataloader):\n",
        "    images, _ = batch\n",
        "    images = images.to(device)\n",
        "    indices = model.compute_indices(images)\n",
        "    celeba_indices.append(indices.view(images.shape[0], LATENT_DIM).cpu())\n",
        "\n",
        "print('Concatenating...')\n",
        "celeba_indices = torch.cat(celeba_indices, dim=0)\n",
        "print('Shape:', celeba_indices.shape)\n",
        "celeba_indices = celeba_indices.to(torch.uint16)\n",
        "print('Saving...')\n",
        "torch.save(celeba_indices, os.path.join(CHECKPOINT_DIR, \"celeba_vq_indices_uint16.pt\"))"
      ],
      "metadata": {
        "id": "x4cvj7nkNtVk"
      },
      "execution_count": null,
      "outputs": [],
      "id": "x4cvj7nkNtVk"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}