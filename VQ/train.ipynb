{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7fbbafb2",
      "metadata": {
        "id": "7fbbafb2"
      },
      "source": [
        "# Training Notebook for VQ-VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b52c75",
      "metadata": {
        "id": "e5b52c75"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R-nf_hUh88R5",
      "metadata": {
        "id": "R-nf_hUh88R5"
      },
      "source": [
        "### Load CelebA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "URPH1QoEOcWg",
      "metadata": {
        "id": "URPH1QoEOcWg"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gWJUEtAeQhhZ",
      "metadata": {
        "id": "gWJUEtAeQhhZ"
      },
      "outputs": [],
      "source": [
        "# access the kaggle.json API key from the main folder of your google drive\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# download the dataset from kaggle\n",
        "!kaggle datasets download -d zuozhaorui/celeba\n",
        "!mkdir ./data\n",
        "!unzip -q celeba.zip -d ./data/celeba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AKe80mkz498P",
      "metadata": {
        "id": "AKe80mkz498P"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "class CelebATransform:\n",
        "    def __call__(self, img):\n",
        "        img = torchvision.transforms.functional.crop(img, top=60, left=25, height=128, width=128)\n",
        "        img = torchvision.transforms.functional.resize(img, (64, 64))\n",
        "        img = torchvision.transforms.functional.to_tensor(img)\n",
        "        return img\n",
        "celeba = torchvision.datasets.ImageFolder(root='./data/celeba', transform=CelebATransform())\n",
        "\n",
        "# visualize\n",
        "grid_x = 5\n",
        "grid_y = 4\n",
        "\n",
        "samples = torch.stack([celeba[i][0] for i in range(grid_x*grid_y)])\n",
        "\n",
        "img = torchvision.utils.make_grid(samples, grid_x, normalize=True)\n",
        "plt.title(f'Sample Images')\n",
        "plt.axis('off')\n",
        "plt.imshow(img.permute(1,2,0).cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4WpiSXGJ8193",
      "metadata": {
        "id": "4WpiSXGJ8193"
      },
      "source": [
        "### Import model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LnTDvW2mFqJY",
      "metadata": {
        "id": "LnTDvW2mFqJY"
      },
      "outputs": [],
      "source": [
        "# clone the github repository containing the VQ-VAE model\n",
        "!git clone https://github.com/patrickmastorga/VQ-VAE-Tranformer-Image-Gen.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3612dc73",
      "metadata": {
        "id": "3612dc73"
      },
      "outputs": [],
      "source": [
        "# import VQ-VAE model from model.py\n",
        "os.chdir('VQ-VAE-Tranformer-Image-Gen/VQ')\n",
        "from model import Encoder, Decoder, Quantizer, VQ_VAE, EMBEDDING_DIM\n",
        "os.chdir('../../')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tr-m5WkYRlx7",
      "metadata": {
        "id": "Tr-m5WkYRlx7"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pg7sWQCJ3IZ5",
      "metadata": {
        "id": "pg7sWQCJ3IZ5"
      },
      "outputs": [],
      "source": [
        "# initialize dataloader, models, and optimizer for training\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 1\n",
        "BETA = 0.25\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(celeba, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder().to(device)\n",
        "quantizer = Quantizer().to(device)\n",
        "vq_vae = VQ_VAE(encoder, decoder, quantizer).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(vq_vae.parameters(), lr=1e-3)\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Eps9W3v06jRc",
      "metadata": {
        "collapsed": true,
        "id": "Eps9W3v06jRc"
      },
      "outputs": [],
      "source": [
        "training_losses = []\n",
        "log_interval = 128\n",
        "\n",
        "vq_vae.train()\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        # training step\n",
        "        optimizer.zero_grad()\n",
        "        images, _ = batch\n",
        "        images = images.to(device)\n",
        "        reconstructed, codebook_loss, commitment_loss = vq_vae(images)\n",
        "\n",
        "        recon_loss = criterion(reconstructed, images)\n",
        "        loss = recon_loss + codebook_loss + BETA * commitment_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # keep track of loss and epoch progress\n",
        "        if batch_idx % log_interval == log_interval - 1:\n",
        "            training_losses.append(running_loss / log_interval)\n",
        "            running_loss = 0.0\n",
        "            print(f'TRAINING Epoch [{epoch+1}/{EPOCHS}], Batch [{batch_idx+1}/{len(dataloader)}]], Loss: {training_losses[-1]: .1f}')\n",
        "\n",
        "    # visualize reconstructions\n",
        "    samples, _ = next(iter(dataloader))\n",
        "    samples = samples[:5].to(device)\n",
        "\n",
        "    vq_vae.eval()\n",
        "    with torch.no_grad():\n",
        "        reconstructed, _, _ = vq_vae(samples)\n",
        "\n",
        "    img = torchvision.utils.make_grid(torch.cat((samples, reconstructed), dim=0), 5, normalize=True)\n",
        "    plt.title(f'Reconstructions')\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img.permute(1,2,0).cpu())\n",
        "\n",
        "print(f'Training complete.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
